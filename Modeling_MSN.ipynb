{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9djgAYSqCQ4H",
        "outputId": "b7918a45-9d8c-45a5-e8e3-dba00a5d2b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=d2b4ee78bbca4c23af6d8bf0103e96938e4991d126a184d5b774ff3a180ab3cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1sbDHIaBsy1",
        "outputId": "5e3124e3-2f7b-41bc-d9af-6bb24f9ddf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "339/339 - 15s - 44ms/step - loss: 0.2144 - mean_absolute_error: 0.2144 - val_loss: 0.1136 - val_mean_absolute_error: 0.1136 - learning_rate: 5.0000e-05\n",
            "Epoch 2/50\n",
            "339/339 - 8s - 25ms/step - loss: 0.1523 - mean_absolute_error: 0.1523 - val_loss: 0.1103 - val_mean_absolute_error: 0.1103 - learning_rate: 5.0000e-05\n",
            "Epoch 3/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1497 - mean_absolute_error: 0.1497 - val_loss: 0.1089 - val_mean_absolute_error: 0.1089 - learning_rate: 5.0000e-05\n",
            "Epoch 4/50\n",
            "339/339 - 4s - 11ms/step - loss: 0.1491 - mean_absolute_error: 0.1491 - val_loss: 0.1079 - val_mean_absolute_error: 0.1078 - learning_rate: 5.0000e-05\n",
            "Epoch 5/50\n",
            "339/339 - 5s - 14ms/step - loss: 0.1487 - mean_absolute_error: 0.1486 - val_loss: 0.1071 - val_mean_absolute_error: 0.1071 - learning_rate: 5.0000e-05\n",
            "Epoch 6/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1486 - mean_absolute_error: 0.1486 - val_loss: 0.1065 - val_mean_absolute_error: 0.1065 - learning_rate: 5.0000e-05\n",
            "Epoch 7/50\n",
            "339/339 - 4s - 11ms/step - loss: 0.1484 - mean_absolute_error: 0.1484 - val_loss: 0.1062 - val_mean_absolute_error: 0.1062 - learning_rate: 5.0000e-05\n",
            "Epoch 8/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1481 - mean_absolute_error: 0.1481 - val_loss: 0.1059 - val_mean_absolute_error: 0.1059 - learning_rate: 5.0000e-05\n",
            "Epoch 9/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1480 - mean_absolute_error: 0.1479 - val_loss: 0.1056 - val_mean_absolute_error: 0.1056 - learning_rate: 5.0000e-05\n",
            "Epoch 10/50\n",
            "339/339 - 6s - 17ms/step - loss: 0.1480 - mean_absolute_error: 0.1480 - val_loss: 0.1054 - val_mean_absolute_error: 0.1054 - learning_rate: 5.0000e-05\n",
            "Epoch 11/50\n",
            "339/339 - 4s - 13ms/step - loss: 0.1479 - mean_absolute_error: 0.1478 - val_loss: 0.1052 - val_mean_absolute_error: 0.1052 - learning_rate: 5.0000e-05\n",
            "Epoch 12/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1477 - mean_absolute_error: 0.1477 - val_loss: 0.1052 - val_mean_absolute_error: 0.1052 - learning_rate: 5.0000e-05\n",
            "Epoch 13/50\n",
            "339/339 - 4s - 11ms/step - loss: 0.1477 - mean_absolute_error: 0.1477 - val_loss: 0.1048 - val_mean_absolute_error: 0.1048 - learning_rate: 5.0000e-05\n",
            "Epoch 14/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1477 - mean_absolute_error: 0.1477 - val_loss: 0.1048 - val_mean_absolute_error: 0.1047 - learning_rate: 5.0000e-05\n",
            "Epoch 15/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045 - learning_rate: 5.0000e-05\n",
            "Epoch 16/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1477 - mean_absolute_error: 0.1476 - val_loss: 0.1046 - val_mean_absolute_error: 0.1046 - learning_rate: 5.0000e-05\n",
            "Epoch 17/50\n",
            "339/339 - 5s - 15ms/step - loss: 0.1476 - mean_absolute_error: 0.1476 - val_loss: 0.1043 - val_mean_absolute_error: 0.1043 - learning_rate: 5.0000e-05\n",
            "Epoch 18/50\n",
            "339/339 - 3s - 9ms/step - loss: 0.1475 - mean_absolute_error: 0.1475 - val_loss: 0.1043 - val_mean_absolute_error: 0.1043 - learning_rate: 5.0000e-05\n",
            "Epoch 19/50\n",
            "339/339 - 6s - 18ms/step - loss: 0.1474 - mean_absolute_error: 0.1474 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045 - learning_rate: 5.0000e-05\n",
            "Epoch 20/50\n",
            "339/339 - 3s - 10ms/step - loss: 0.1474 - mean_absolute_error: 0.1474 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045 - learning_rate: 5.0000e-05\n",
            "Epoch 21/50\n",
            "339/339 - 3s - 9ms/step - loss: 0.1474 - mean_absolute_error: 0.1474 - val_loss: 0.1045 - val_mean_absolute_error: 0.1045 - learning_rate: 2.5000e-05\n",
            "Epoch 22/50\n",
            "339/339 - 4s - 11ms/step - loss: 0.1474 - mean_absolute_error: 0.1474 - val_loss: 0.1044 - val_mean_absolute_error: 0.1044 - learning_rate: 2.5000e-05\n",
            "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
            "Step 1 MAE = 0.1269\n",
            "Step 2 MAE = 0.1458\n",
            "Step 3 MAE = 0.1628\n",
            "Step 4 MAE = 0.1755\n",
            "Step 5 MAE = 0.1900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tar_s.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import ta\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from tensorflow.keras import mixed_precision, optimizers, callbacks\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, LayerNormalization,\n",
        "    Conv1D, Add, GlobalAveragePooling1D,\n",
        "    Concatenate\n",
        ")\n",
        "\n",
        "# === 0. TF CONFIGS ===\n",
        "tf.config.optimizer.set_jit(True)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# === 1. LOAD & CLEAN DATA ===\n",
        "df = pd.read_csv('/content/MSN.csv', parse_dates=['Date/Time'], on_bad_lines='skip')\n",
        "df = (\n",
        "    df.drop(columns=['Ticker', 'Open Interest'], errors='ignore')\n",
        "      .sort_values('Date/Time')\n",
        "      .ffill().bfill().dropna().reset_index(drop=True)\n",
        ")\n",
        "\n",
        "df['hour'] = df['Date/Time'].dt.hour.astype('int32')\n",
        "df['roll_mean_30'] = df['Close'].rolling(30).mean().astype('float32')\n",
        "df['roll_std_30']  = df['Close'].rolling(30).std().astype('float32')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'roll_mean_30', 'roll_std_30']:\n",
        "    df[col] = df[col].astype('float32')\n",
        "\n",
        "# === 2. INDICATORS & LAGS ===\n",
        "lags = [1, 2, 3, 5]\n",
        "indicators = {\n",
        "    'RSI': lambda x: ta.momentum.RSIIndicator(x).rsi(),\n",
        "    'BBw': lambda x: ta.volatility.BollingerBands(x).bollinger_wband(),\n",
        "    'MACD': lambda x: ta.trend.macd_diff(x),\n",
        "    'EMA20': lambda x: ta.trend.ema_indicator(x, window=20),\n",
        "    'ATR': lambda h, l, c: ta.volatility.AverageTrueRange(h, l, c).average_true_range(),\n",
        "    'ADX': lambda h, l, c: ta.trend.adx(h, l, c),\n",
        "    'OBV': lambda x, v: ta.volume.on_balance_volume(x, v)\n",
        "}\n",
        "\n",
        "for name, fn in indicators.items():\n",
        "    if name == 'OBV':\n",
        "        df[name] = fn(df['Close'], df['Volume']).astype('float32')\n",
        "    elif name in ['ATR', 'ADX']:\n",
        "        df[name] = fn(df['High'], df['Low'], df['Close']).astype('float32')\n",
        "    else:\n",
        "        df[name] = fn(df['Close']).astype('float32')\n",
        "\n",
        "for lag in lags:\n",
        "    df[f'C_lag{lag}'] = df['Close'].shift(lag)\n",
        "    df[f'V_lag{lag}'] = df['Volume'].shift(lag)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# === 3. MULTI-STEP TARGET ===\n",
        "H = 5\n",
        "for h in range(1, H + 1):\n",
        "    df[f'Fut{h}'] = df['Close'].shift(-h)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "Y_all = np.stack([\n",
        "    df[f'Fut{h}'].to_numpy() - df['Close'].to_numpy() for h in range(1, H + 1)\n",
        "], axis=1).astype('float32')\n",
        "\n",
        "# === 4. FEATURE LISTS ===\n",
        "static_feats = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'roll_mean_30', 'roll_std_30', 'hour',\n",
        "    *indicators.keys(),\n",
        "    *[f'C_lag{lag}' for lag in lags],\n",
        "    *[f'V_lag{lag}' for lag in lags]\n",
        "]\n",
        "\n",
        "seq_feats = ['Close', 'Volume', 'RSI', 'BBw', 'MACD', 'EMA20', 'ATR', 'ADX', 'OBV']\n",
        "window = 30\n",
        "\n",
        "# === 5. BUILD SEQUENCES ===\n",
        "def build_seq(df, window):\n",
        "    arr_s = df[static_feats].to_numpy(dtype='float32')\n",
        "    arr_q = df[seq_feats].to_numpy(dtype='float32')\n",
        "    S, Q = [], []\n",
        "    for i in range(window, len(df) - H + 1):\n",
        "        S.append(arr_s[i])\n",
        "        Q.append(arr_q[i - window:i])\n",
        "    return np.array(S), np.array(Q)\n",
        "\n",
        "Xs, Xq = build_seq(df, window)\n",
        "N = Xs.shape[0]\n",
        "Y = Y_all[window:window + N]\n",
        "\n",
        "# === 6. SPLIT & SCALE ===\n",
        "iT = int(N * 0.8)\n",
        "iV = int(iT * 0.8)\n",
        "\n",
        "Xs_tr, Xs_val, Xs_te = Xs[:iV], Xs[iV:iT], Xs[iT:]\n",
        "Xq_tr, Xq_val, Xq_te = Xq[:iV], Xq[iV:iT], Xq[iT:]\n",
        "Y_tr, Y_val, Y_te = Y[:iV], Y[iV:iT], Y[iT:]\n",
        "\n",
        "scaler_s = StandardScaler().fit(Xs_tr)\n",
        "scaler_q = StandardScaler().fit(Xq_tr.reshape(-1, len(seq_feats)))\n",
        "scaler_tar = RobustScaler().fit(Y_tr)\n",
        "\n",
        "def scale(Xs, Xq):\n",
        "    xs = scaler_s.transform(Xs)\n",
        "    xq = scaler_q.transform(Xq.reshape(-1, len(seq_feats))).reshape(Xq.shape)\n",
        "    return xs, xq\n",
        "\n",
        "Xs_tr_s, Xq_tr_s = scale(Xs_tr, Xq_tr)\n",
        "Xs_val_s, Xq_val_s = scale(Xs_val, Xq_val)\n",
        "Xs_te_s, Xq_te_s = scale(Xs_te, Xq_te)\n",
        "\n",
        "# === 7. TF DATASETS ===\n",
        "bs = 256\n",
        "def mkds(Xs, Xq, Y, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((Xs, Xq), Y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=10000)\n",
        "    return ds.batch(bs).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_tr = mkds(Xs_tr_s, Xq_tr_s, Y_tr, True)\n",
        "ds_val = mkds(Xs_val_s, Xq_val_s, Y_val)\n",
        "\n",
        "# === 8. TCN BLOCK ===\n",
        "def TCN(x, filters, kernel, dil):\n",
        "    y = Conv1D(filters, kernel, dilation_rate=dil, padding='causal', activation='relu')(x)\n",
        "    y = Conv1D(filters, kernel, dilation_rate=dil, padding='causal', activation='relu')(y)\n",
        "    return Add()([x, y])\n",
        "\n",
        "# === 9. MODEL ===\n",
        "with strategy.scope():\n",
        "    def build():\n",
        "        inp_s = Input((len(static_feats),), name='static_input')\n",
        "        xs = Dense(64, activation='relu')(inp_s)\n",
        "        xs = Dropout(0.3)(xs)\n",
        "\n",
        "        inp_q = Input((window, len(seq_feats)), name='seq_input')\n",
        "        x = Dense(128)(inp_q)\n",
        "        for d in [1, 2, 4, 8, 16]:\n",
        "            x = TCN(x, 128, 3, d)\n",
        "        x = LayerNormalization()(x)\n",
        "        enc = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        merged = Concatenate()([xs, enc])\n",
        "        h = Dense(128, activation='relu')(merged)\n",
        "        h = Dropout(0.2)(h)\n",
        "        out = Dense(H, name='reg')(h)\n",
        "\n",
        "        model = Model([inp_s, inp_q], out)\n",
        "        model.compile(\n",
        "            optimizer=optimizers.AdamW(learning_rate=5e-5, weight_decay=1e-4),\n",
        "            loss='mae',\n",
        "            metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    model = build()\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "    hist = model.fit(ds_tr, validation_data=ds_val, epochs=50, callbacks=[es, rl], verbose=2)\n",
        "\n",
        "# === 10. EVALUATE ===\n",
        "pred = model.predict([Xs_te_s, Xq_te_s], batch_size=bs)\n",
        "pred = scaler_tar.inverse_transform(pred)\n",
        "\n",
        "for h in range(H):\n",
        "    print(f\"Step {h+1} MAE = {mean_absolute_error(Y_te[:, h], pred[:, h]):.4f}\")\n",
        "\n",
        "# === 11. SAVE MODEL ===\n",
        "model.save('best_tcn_enhanced.keras')\n",
        "joblib.dump(scaler_s, 'stat_s.pkl')\n",
        "joblib.dump(scaler_q, 'seq_s.pkl')\n",
        "joblib.dump(scaler_tar, 'tar_s.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}