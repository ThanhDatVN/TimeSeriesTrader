{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATHnLojzQl2E",
        "outputId": "7154cf7a-572c-4d1d-f554-4523ecf31ae5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6mJSfeAftbW",
        "outputId": "bc411d76-5147-431a-b746-f9b49c47d6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(89934, 15)\n",
            "(89934, 30, 15)\n",
            "(89934,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:28:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 29279, number of negative: 30677\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3825\n",
            "[LightGBM] [Info] Number of data points in the train set: 59956, number of used features: 15\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488341 -> initscore=-0.046643\n",
            "[LightGBM] [Info] Start training from score -0.046643\n",
            "Epoch 1/50\n",
            "1874/1874 - 22s - 12ms/step - accuracy: 0.6179 - loss: 0.0484 - val_accuracy: 0.5366 - val_loss: 0.0466 - learning_rate: 1.0000e-03\n",
            "Epoch 2/50\n",
            "1874/1874 - 18s - 10ms/step - accuracy: 0.6421 - loss: 0.0417 - val_accuracy: 0.5257 - val_loss: 0.0444 - learning_rate: 1.0000e-03\n",
            "Epoch 3/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6475 - loss: 0.0401 - val_accuracy: 0.5357 - val_loss: 0.0439 - learning_rate: 1.0000e-03\n",
            "Epoch 4/50\n",
            "1874/1874 - 20s - 11ms/step - accuracy: 0.6479 - loss: 0.0396 - val_accuracy: 0.5373 - val_loss: 0.0436 - learning_rate: 1.0000e-03\n",
            "Epoch 5/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6524 - loss: 0.0394 - val_accuracy: 0.5291 - val_loss: 0.0433 - learning_rate: 1.0000e-03\n",
            "Epoch 6/50\n",
            "1874/1874 - 20s - 11ms/step - accuracy: 0.6508 - loss: 0.0393 - val_accuracy: 0.5462 - val_loss: 0.0431 - learning_rate: 1.0000e-03\n",
            "Epoch 7/50\n",
            "1874/1874 - 18s - 10ms/step - accuracy: 0.6489 - loss: 0.0391 - val_accuracy: 0.5463 - val_loss: 0.0434 - learning_rate: 1.0000e-03\n",
            "Epoch 8/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6506 - loss: 0.0392 - val_accuracy: 0.5339 - val_loss: 0.0432 - learning_rate: 1.0000e-03\n",
            "Epoch 9/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6511 - loss: 0.0391 - val_accuracy: 0.5236 - val_loss: 0.0432 - learning_rate: 1.0000e-03\n",
            "Epoch 10/50\n",
            "1874/1874 - 20s - 11ms/step - accuracy: 0.6538 - loss: 0.0389 - val_accuracy: 0.5308 - val_loss: 0.0434 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "1874/1874 - 19s - 10ms/step - accuracy: 0.6543 - loss: 0.0389 - val_accuracy: 0.5391 - val_loss: 0.0431 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6555 - loss: 0.0388 - val_accuracy: 0.5384 - val_loss: 0.0433 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6547 - loss: 0.0388 - val_accuracy: 0.5440 - val_loss: 0.0431 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "1874/1874 - 19s - 10ms/step - accuracy: 0.6570 - loss: 0.0387 - val_accuracy: 0.5446 - val_loss: 0.0429 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "1874/1874 - 21s - 11ms/step - accuracy: 0.6576 - loss: 0.0387 - val_accuracy: 0.5601 - val_loss: 0.0431 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "1874/1874 - 22s - 12ms/step - accuracy: 0.6562 - loss: 0.0386 - val_accuracy: 0.5394 - val_loss: 0.0432 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "1874/1874 - 22s - 12ms/step - accuracy: 0.6574 - loss: 0.0385 - val_accuracy: 0.5495 - val_loss: 0.0431 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "1874/1874 - 38s - 20ms/step - accuracy: 0.6602 - loss: 0.0385 - val_accuracy: 0.5483 - val_loss: 0.0429 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "1874/1874 - 19s - 10ms/step - accuracy: 0.6585 - loss: 0.0386 - val_accuracy: 0.5427 - val_loss: 0.0431 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "[XGB] Accuracy: 0.5233838147975182\n",
            "[[2131 5468]\n",
            " [1676 5714]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.28      0.37      7599\n",
            "           1       0.51      0.77      0.62      7390\n",
            "\n",
            "    accuracy                           0.52     14989\n",
            "   macro avg       0.54      0.53      0.49     14989\n",
            "weighted avg       0.54      0.52      0.49     14989\n",
            "\n",
            "[LGBM] Accuracy: 0.5268530255520715\n",
            "[[1744 5855]\n",
            " [1237 6153]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.23      0.33      7599\n",
            "           1       0.51      0.83      0.63      7390\n",
            "\n",
            "    accuracy                           0.53     14989\n",
            "   macro avg       0.55      0.53      0.48     14989\n",
            "weighted avg       0.55      0.53      0.48     14989\n",
            "\n",
            "[NN] Accuracy: 0.5628127293348456\n",
            "[[2312 5287]\n",
            " [1266 6124]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.30      0.41      7599\n",
            "           1       0.54      0.83      0.65      7390\n",
            "\n",
            "    accuracy                           0.56     14989\n",
            "   macro avg       0.59      0.57      0.53     14989\n",
            "weighted avg       0.59      0.56      0.53     14989\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, GRU, Dense, Dropout,\n",
        "    BatchNormalization, Concatenate\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers, mixed_precision\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Setup\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/PNJ.csv', parse_dates=['Date/Time'], on_bad_lines='skip')\n",
        "df = (df.drop(columns=['Ticker','Open Interest'], errors='ignore')\n",
        "        .sort_values('Date/Time').ffill().bfill().dropna()\n",
        "        .reset_index(drop=True))\n",
        "for col in ['Open','High','Low','Close','Volume']:\n",
        "    df[col] = df[col].astype('float32')\n",
        "\n",
        "# Technical indicators\n",
        "def compute_rsi(s, length=14):\n",
        "    delta = s.diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_g = gain.ewm(alpha=1/length).mean()\n",
        "    avg_l = loss.ewm(alpha=1/length).mean()\n",
        "    rs = avg_g/(avg_l+1e-8)\n",
        "    return 100 - 100/(1+rs)\n",
        "\n",
        "def compute_bb_width(s, length=20, dev=2.0):\n",
        "    m = s.rolling(length).mean()\n",
        "    sd = s.rolling(length).std()\n",
        "    return (m + dev*sd) - (m - dev*sd)\n",
        "\n",
        "def compute_macd(s, fast=12, slow=26, signal=9):\n",
        "    ema_fast = s.ewm(span=fast).mean()\n",
        "    ema_slow = s.ewm(span=slow).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    sig = macd.ewm(span=signal).mean()\n",
        "    return macd - sig\n",
        "\n",
        "def compute_atr(df, length=14):\n",
        "    hl = df['High'] - df['Low']\n",
        "    hc = (df['High'] - df['Close'].shift()).abs()\n",
        "    lc = (df['Low'] - df['Close'].shift()).abs()\n",
        "    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)\n",
        "    return tr.rolling(length).mean()\n",
        "\n",
        "window_sizes = [5, 10, 20]\n",
        "for w in window_sizes:\n",
        "    df[f'roll_mean_{w}'] = df['Close'].rolling(w).mean()\n",
        "    df[f'roll_std_{w}'] = df['Close'].rolling(w).std()\n",
        "\n",
        "df['RSI'] = compute_rsi(df['Close']).astype('float32')\n",
        "df['BB_w'] = compute_bb_width(df['Close']).astype('float32')\n",
        "df['MACD'] = compute_macd(df['Close']).astype('float32')\n",
        "df['ATR'] = compute_atr(df).astype('float32')\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Classification target\n",
        "df['Future'] = df['Close'].shift(-5)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "df['LogRet'] = np.log(df['Future'] / df['Close'])\n",
        "df['Direction'] = (df['LogRet'] > 0).astype('int')\n",
        "df = df[df['LogRet'].abs() >= 0.001].reset_index(drop=True)\n",
        "\n",
        "# Features\n",
        "static_feats = ['Open','High','Low','Close','Volume','RSI','BB_w','MACD','ATR']\n",
        "static_feats += [f'roll_mean_{w}' for w in window_sizes] + [f'roll_std_{w}' for w in window_sizes]\n",
        "seq_feats = static_feats\n",
        "\n",
        "X = df[static_feats].to_numpy(dtype='float32')\n",
        "Y = df['Direction'].to_numpy(dtype='int')\n",
        "S = df[seq_feats].to_numpy(dtype='float32')\n",
        "\n",
        "W = 30\n",
        "\n",
        "def make_seq(X, S, Y, W):\n",
        "    xs, ss, ys = [], [], []\n",
        "    for i in range(W, len(X)):\n",
        "        xs.append(X[i])\n",
        "        ss.append(S[i-W:i])\n",
        "        ys.append(Y[i])\n",
        "    return np.array(xs), np.array(ss), np.array(ys)\n",
        "\n",
        "Xs, Ss, Ys = make_seq(X, S, Y, W)\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(Xs.shape)  # Shape of Xs\n",
        "print(Ss.shape)  # Shape of Ss\n",
        "print(Ys.shape)  # Shape of Ys\n",
        "\n",
        "# Ensure all arrays have the same length\n",
        "assert Xs.shape[0] == Ss.shape[0] == Ys.shape[0], \"Array sizes do not match!\"\n",
        "\n",
        "# Split\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "splits = list(tscv.split(Xs))\n",
        "tr_idx, te_idx = splits[-1]\n",
        "val_cut = int(0.8 * len(tr_idx))\n",
        "tr_idx, val_idx = tr_idx[:val_cut], tr_idx[val_cut:]\n",
        "\n",
        "X_tr, X_val, X_te = Xs[tr_idx], Xs[val_idx], Xs[te_idx]\n",
        "S_tr, S_val, S_te = Ss[tr_idx], Ss[val_idx], Ss[te_idx]\n",
        "y_tr, y_val, y_te = Ys[tr_idx], Ys[val_idx], Ys[te_idx]\n",
        "\n",
        "# Scaling\n",
        "stat_s = StandardScaler().fit(X_tr)\n",
        "seq_s = StandardScaler().fit(S_tr.reshape(-1, len(seq_feats)))\n",
        "X_tr_s, X_val_s, X_te_s = stat_s.transform(X_tr), stat_s.transform(X_val), stat_s.transform(X_te)\n",
        "S_tr_s = seq_s.transform(S_tr.reshape(-1, len(seq_feats))).reshape(S_tr.shape)\n",
        "S_val_s = seq_s.transform(S_val.reshape(-1, len(seq_feats))).reshape(S_val.shape)\n",
        "S_te_s = seq_s.transform(S_te.reshape(-1, len(seq_feats))).reshape(S_te.shape)\n",
        "\n",
        "# Feature Selection\n",
        "selector = SelectKBest(f_classif, k=15)\n",
        "X_tr_s_fs = selector.fit_transform(X_tr_s, y_tr)\n",
        "X_val_s_fs = selector.transform(X_val_s)\n",
        "X_te_s_fs = selector.transform(X_te_s)\n",
        "\n",
        "# Class weights\n",
        "cw = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
        "cw_dict = dict(enumerate(cw))\n",
        "\n",
        "# Tree-based models\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=cw[1]/cw[0])\n",
        "lgbm = LGBMClassifier(random_state=42, scale_pos_weight=cw[1]/cw[0])\n",
        "xgb.fit(X_tr_s_fs, y_tr)\n",
        "lgbm.fit(X_tr_s_fs, y_tr)\n",
        "\n",
        "# NN\n",
        "\n",
        "def focal_loss(gamma=2., alpha=.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
        "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        return -tf.reduce_mean(alpha * tf.pow(1. - pt, gamma) * tf.math.log(pt + epsilon))\n",
        "    return loss\n",
        "\n",
        "def build_nn_classifier():\n",
        "    i1 = Input((X_tr_s.shape[1],))\n",
        "    x1 = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(i1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Dropout(0.4)(x1)\n",
        "\n",
        "    i2 = Input((W, S_tr_s.shape[2]))\n",
        "    x2 = Conv1D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-4))(i2)\n",
        "    x2 = Dropout(0.4)(x2)\n",
        "    x2 = GRU(32, kernel_regularizer=regularizers.l2(1e-4))(x2)\n",
        "\n",
        "    m = Concatenate()([x1, x2])\n",
        "    m = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(m)\n",
        "    m = Dropout(0.4)(m)\n",
        "    o = Dense(1, activation='sigmoid')(m)\n",
        "\n",
        "    model = Model([i1, i2], o)\n",
        "    model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn = build_nn_classifier()\n",
        "nn.fit([X_tr_s, S_tr_s], y_tr,\n",
        "       validation_data=([X_val_s, S_val_s], y_val),\n",
        "       epochs=50, batch_size=32,\n",
        "       class_weight=cw_dict,\n",
        "       callbacks=[EarlyStopping('val_loss', patience=5, restore_best_weights=True),\n",
        "                  ReduceLROnPlateau('val_loss', factor=0.5, patience=3)],\n",
        "       verbose=2)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_classifiers(name, y_true, y_pred):\n",
        "    print(f'[{name}] Accuracy:', accuracy_score(y_true, y_pred))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "preds_xgb = xgb.predict(X_te_s_fs)\n",
        "preds_lgbm = lgbm.predict(X_te_s_fs)\n",
        "preds_nn = (nn.predict([X_te_s, S_te_s]) > 0.5).astype(int).flatten()\n",
        "\n",
        "for name, preds in [('XGB', preds_xgb), ('LGBM', preds_lgbm), ('NN', preds_nn)]:\n",
        "    evaluate_classifiers(name, y_te, preds)\n",
        "\n",
        "# Save models\n",
        "nn.save('model_clf.keras', include_optimizer=False)\n",
        "joblib.dump(stat_s, 'stat_clf.pkl')\n",
        "joblib.dump(seq_s, 'seq_clf.pkl')\n",
        "joblib.dump(xgb, 'xgb_clf.pkl')\n",
        "joblib.dump(lgbm, 'lgbm_clf.pkl')\n",
        "print('Done.')\n"
      ]
    }
  ]
}